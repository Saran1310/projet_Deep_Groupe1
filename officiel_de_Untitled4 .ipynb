{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DA8OJ8r28pkY"
      },
      "outputs": [],
      "source": [
        "!pip -q install kaggle"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "from google.colab import userdata\n",
        "\n",
        "# 1. Récupération sécurisée des identifiants (Assurez-vous d'avoir créé les Secrets dans Colab)\n",
        "os.environ['KAGGLE_USERNAME'] = userdata.get('KAGGLE_USERNAME')\n",
        "os.environ['KAGGLE_KEY'] = userdata.get('KAGGLE_KEY')\n",
        "\n",
        "# 2. Définition du chemin et création du répertoire\n",
        "kaggle_path = os.path.expanduser(\"~/.kaggle\")\n",
        "os.makedirs(kaggle_path, exist_ok=True)\n",
        "\n",
        "# 3. Création du fichier kaggle.json [cite: 31]\n",
        "with open(os.path.join(kaggle_path, \"kaggle.json\"), \"w\") as f:\n",
        "    json.dump({\n",
        "        \"username\": os.environ['KAGGLE_USERNAME'],\n",
        "        \"key\": os.environ['KAGGLE_KEY']\n",
        "    }, f)\n",
        "\n",
        "# 4. Sécurisation du fichier [cite: 32]\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "\n",
        "# 5. Téléchargement du dataset [cite: 34]\n",
        "!kaggle datasets download -d chetankv/dogs-cats-images -p data/ --unzip\n",
        "\n",
        "# 6. Vérification du contenu téléchargé\n",
        "print(\"\\nDossiers téléchargés :\")\n",
        "!ls data/dog\\ vs\\ cat/dataset"
      ],
      "metadata": {
        "id": "XYFfk6X2JzCL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn as nn\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "SEED = 42\n",
        "\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed(SEED)\n",
        "    torch.cuda.manual_seed_all(SEED)\n",
        "    device = \"cuda\"\n",
        "else:\n",
        "    device = \"cpu\"\n",
        "\n",
        "\n",
        "# 1. Définition des transformations (Point crucial pour l'amélioration) [cite: 56]\n",
        "data_transform = transforms.Compose([\n",
        "    transforms.Resize(size=(64, 64)), # Taille standard pour le modèle de base [cite: 53]\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "# 2. Chargement via ImageFolder [cite: 17, 41]\n",
        "train_dir = \"data/dog vs cat/dataset/training_set\"\n",
        "test_dir = \"data/dog vs cat/dataset/test_set\"\n",
        "\n",
        "train_data = datasets.ImageFolder(root=train_dir, transform=data_transform)\n",
        "test_data = datasets.ImageFolder(root=test_dir, transform=data_transform)\n",
        "\n",
        "train_loader = DataLoader(dataset=train_data, batch_size=32, shuffle=True)\n",
        "test_loader = DataLoader(dataset=test_data, batch_size=32, shuffle=False)\n",
        "\n",
        "print(f\"Dataset prêt ! Classes trouvées : {train_data.classes}\")\n",
        "print(f\"Nombre de donnée train : {len(train_data)}\")\n",
        "print(f\"Nombre de donnée test : {len(test_data)}\")"
      ],
      "metadata": {
        "id": "8JVaoPoGRbz7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleCNN_AvgPool(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleCNN_AvgPool, self).__init__()\n",
        "        # Couche 1\n",
        "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, padding=1)\n",
        "        self.act1 = nn.ReLU()\n",
        "        self.pool1 = nn.AvgPool2d(kernel_size=2) # On prend la moyenne au lieu du max\n",
        "\n",
        "        # Couche 2\n",
        "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n",
        "        self.act2 = nn.ReLU()\n",
        "        self.pool2 = nn.AvgPool2d(kernel_size=2) # On prend la moyenne\n",
        "\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.fc = nn.Linear(32 * 16 * 16, 2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool1(self.act1(self.conv1(x)))\n",
        "        x = self.pool2(self.act2(self.conv2(x)))\n",
        "        x = self.flatten(x)\n",
        "        return self.fc(x)\n",
        "\n",
        "model_base = SimpleCNN_AvgPool().to(device)"
      ],
      "metadata": {
        "id": "PQWI68OfRx1i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(params=model_base.parameters(), lr=0.01)"
      ],
      "metadata": {
        "id": "IJjVOGdpTY_2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "# 1. Configuration de l'entraînement\n",
        "epochs = 10\n",
        "train_losses = []\n",
        "test_accuracies = []\n",
        "\n",
        "print(f\"Début de l'entraînement sur {device}...\")\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    start_time = time.time()\n",
        "\n",
        "    # --- PHASE D'ENTRAÎNEMENT ---\n",
        "    model_base.train()\n",
        "    train_loss = 0\n",
        "\n",
        "    for batch, (X, y) in enumerate(train_loader):\n",
        "        # Envoi des données sur le GPU/CPU\n",
        "        X, y = X.to(device), y.to(device)\n",
        "\n",
        "        # 1. Forward pass\n",
        "        y_pred = model_base(X)\n",
        "\n",
        "        # 2. Calcul de la perte\n",
        "        loss = loss_fn(y_pred, y)\n",
        "        train_loss += loss.item()\n",
        "\n",
        "        # 3. Optimizer zero grad\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # 4. Loss backward (Backpropagation)\n",
        "        loss.backward()\n",
        "\n",
        "        # 5. Optimizer step\n",
        "        optimizer.step()\n",
        "\n",
        "    # --- PHASE DE TEST (ÉVALUATION) ---\n",
        "    model_base.eval()\n",
        "    test_acc = 0\n",
        "    with torch.inference_mode(): # Mode économie pour le test\n",
        "        for X_test, y_test in test_loader:\n",
        "            X_test, y_test = X_test.to(device), y_test.to(device)\n",
        "            test_pred_logits = model_base(X_test)\n",
        "\n",
        "            # Calcul de la précision\n",
        "            test_pred_labels = torch.argmax(test_pred_logits, dim=1)\n",
        "            test_acc += (test_pred_labels == y_test).sum().item()\n",
        "\n",
        "    # Calcul des moyennes pour l'affichage\n",
        "    train_loss /= len(train_loader)\n",
        "    test_acc /= len(test_data)\n",
        "\n",
        "    train_losses.append(train_loss)\n",
        "    test_accuracies.append(test_acc)\n",
        "\n",
        "    end_time = time.time()\n",
        "    print(f\"Époque: {epoch+1} | Perte: {train_loss:.4f} | Précision Test: {test_acc*100:.2f}% | Temps: {end_time-start_time:.1f}s\")\n",
        "\n",
        "print(\"\\nEntraînement terminé !\")"
      ],
      "metadata": {
        "id": "PqnjABGoTcAu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_and_plot(model, dataset, n_images=5):\n",
        "    model.eval()\n",
        "    plt.figure(figsize=(15, 5))\n",
        "    for i in range(n_images):\n",
        "        # On prend une image au hasard dans le dataset de test\n",
        "        idx = torch.randint(0, len(dataset), (1,)).item()\n",
        "        image, label = dataset[idx]\n",
        "\n",
        "        # Prédiction\n",
        "        with torch.inference_mode():\n",
        "            pred_logits = model(image.unsqueeze(0).to(device))\n",
        "            pred_label = torch.argmax(pred_logits, dim=1).item()\n",
        "\n",
        "        # Affichage\n",
        "        plt.subplot(1, n_images, i+1)\n",
        "        plt.imshow(image.permute(1, 2, 0))\n",
        "        color = \"green\" if pred_label == label else \"red\"\n",
        "        plt.title(f\"Vrai: {dataset.classes[label]}\\nPred: {dataset.classes[pred_label]}\", color=color)\n",
        "        plt.axis(\"off\")\n",
        "    plt.show()\n",
        "\n",
        "predict_and_plot(model_base, test_data)"
      ],
      "metadata": {
        "id": "MbTSeRZtXlV6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class SimpleCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleCNN, self).__init__()\n",
        "        # Couche 1 : Détecte les formes de base (bords, lignes)\n",
        "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, padding=1)\n",
        "        self.act1 = nn.ReLU()\n",
        "        self.pool1 = nn.MaxPool2d(kernel_size=2) # Divise la taille de l'image par 2\n",
        "\n",
        "        # Couche 2 : Détecte des formes plus complexes\n",
        "        self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, padding=1)\n",
        "        self.act2 = nn.ReLU()\n",
        "        self.pool2 = nn.MaxPool2d(kernel_size=2)\n",
        "\n",
        "        # Couche de sortie : Classifie en 2 catégories (Chat ou Chien)\n",
        "        self.flatten = nn.Flatten()\n",
        "        # 16*16 est la taille de l'image après deux passages en MaxPool (64/2/2 = 16)\n",
        "        self.fc = nn.Linear(32 * 16 * 16, 2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool1(self.act1(self.conv1(x)))\n",
        "        x = self.pool2(self.act2(self.conv2(x)))\n",
        "        x = self.flatten(x)\n",
        "        return self.fc(x)\n",
        "\n",
        "# Instanciation et envoi sur le GPU/CPU\n",
        "model_base = SimpleCNN().to(device)\n",
        "# print(model_base)"
      ],
      "metadata": {
        "id": "090c_4pXHmmZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#experience 1\n",
        "model_base = SimpleCNN().to(device)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer=torch.optim.Adam(params=model_base.parameters(), lr=0.0001)\n",
        "\n"
      ],
      "metadata": {
        "id": "L2ZgvDKwXlCY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 10\n",
        "train_losses_adam = []\n",
        "test_accuracies_adam = []\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    model_base.train()\n",
        "    train_loss = 0\n",
        "    for batch, (X, y) in enumerate(train_loader):\n",
        "        X, y = X.to(device), y.to(device)\n",
        "        y_pred = model_base(X)\n",
        "        loss = loss_fn(y_pred, y)\n",
        "        train_loss += loss.item()\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    # Phase de Test\n",
        "    model_base.eval()\n",
        "    test_acc = 0\n",
        "    with torch.inference_mode():\n",
        "        for X_test, y_test in test_loader:\n",
        "            X_test, y_test = X_test.to(device), y_test.to(device)\n",
        "            test_pred_logits = model_base(X_test)\n",
        "            test_acc += (torch.argmax(test_pred_logits, dim=1) == y_test).sum().item()\n",
        "\n",
        "    train_loss /= len(train_loader)\n",
        "    test_acc /= len(test_data)\n",
        "\n",
        "    train_losses_adam.append(train_loss)\n",
        "    test_accuracies_adam.append(test_acc)\n",
        "\n",
        "    print(f\"Époque: {epoch+1} | Perte: {train_loss:.4f} | Précision Test: {test_acc*100:.2f}%\")"
      ],
      "metadata": {
        "id": "i7Ap9Ap1lPvA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- EXPÉRIENCE 2 : DATA AUGMENTATION ---\n",
        "\n",
        "# 1. Transformations d'entraînement (avec augmentation)\n",
        "train_transform_aug = transforms.Compose([\n",
        "    transforms.Resize(size=(64, 64)),\n",
        "    transforms.RandomHorizontalFlip(p=0.5),      # Retourne l'image aléatoirement\n",
        "    transforms.RandomRotation(degrees=15),       # Rotation légère\n",
        "    transforms.ColorJitter(brightness=0.2),      # Change un peu la luminosité\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "# 2. Transformations de test (restent simples)\n",
        "test_transform_simple = transforms.Compose([\n",
        "    transforms.Resize(size=(64, 64)),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "# 3. Chargement des données avec les nouveaux transforms\n",
        "# On utilise les chemins définis dans ton notebook\n",
        "train_dir = \"data/dog vs cat/dataset/training_set\"\n",
        "test_dir = \"data/dog vs cat/dataset/test_set\"\n",
        "\n",
        "train_data_aug = datasets.ImageFolder(root=train_dir, transform=train_transform_aug)\n",
        "test_data_simple = datasets.ImageFolder(root=test_dir, transform=test_transform_simple)\n",
        "\n",
        "# 4. Création des nouveaux DataLoaders\n",
        "train_loader_aug = DataLoader(dataset=train_data_aug, batch_size=32, shuffle=True)\n",
        "test_loader_simple = DataLoader(dataset=test_data_simple, batch_size=32, shuffle=False)\n",
        "\n",
        "print(f\"Data Augmentation activée pour {len(train_data_aug)} images d'entraînement.\")"
      ],
      "metadata": {
        "id": "l3hgU-bx-W-X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# On réinitialise le modèle et l'optimiseur\n",
        "model_aug = SimpleCNN().to(device)\n",
        "optimizer_aug = torch.optim.Adam(params=model_aug.parameters(), lr=0.001)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "epochs = 10\n",
        "train_losses_aug = []\n",
        "test_accuracies_aug = []\n",
        "\n",
        "print(\"Début de l'entraînement avec Data Augmentation...\")\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    model_aug.train()\n",
        "    train_loss = 0\n",
        "    for X, y in train_loader_aug: # Utilisation du loader augmenté\n",
        "        X, y = X.to(device), y.to(device)\n",
        "        y_pred = model_aug(X)\n",
        "        loss = loss_fn(y_pred, y)\n",
        "        train_loss += loss.item()\n",
        "\n",
        "        optimizer_aug.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer_aug.step()\n",
        "\n",
        "    # Évaluation sur le set de test (non augmenté)\n",
        "    model_aug.eval()\n",
        "    test_acc = 0\n",
        "    with torch.inference_mode():\n",
        "        for X_test, y_test in test_loader_simple:\n",
        "            X_test, y_test = X_test.to(device), y_test.to(device)\n",
        "            test_pred = model_aug(X_test)\n",
        "            test_acc += (torch.argmax(test_pred, dim=1) == y_test).sum().item()\n",
        "\n",
        "    train_loss /= len(train_loader_aug)\n",
        "    test_acc /= len(test_data_simple)\n",
        "\n",
        "    train_losses_aug.append(train_loss)\n",
        "    test_accuracies_aug.append(test_acc)\n",
        "\n",
        "    print(f\"Époque: {epoch+1} | Perte: {train_loss:.4f} | Précision Test: {test_acc*100:.2f}%\")"
      ],
      "metadata": {
        "id": "2G9fgZF99Ozb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- EXPÉRIENCE 3 : AJOUT D'UNE 3ème COUCHE DE CONVOLUTION ---\n",
        "\n",
        "\n",
        "class DeepCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(DeepCNN, self).__init__()\n",
        "        # Couche 1 : 64x64 -> 32x32\n",
        "        self.layer1 = nn.Sequential(\n",
        "            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2)\n",
        "        )\n",
        "        # Couche 2 : 32x32 -> 16x16\n",
        "        self.layer2 = nn.Sequential(\n",
        "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2)\n",
        "        )\n",
        "        # Couche 3 (Nouvelle) : 16x16 -> 8x8\n",
        "        self.layer3 = nn.Sequential(\n",
        "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2)\n",
        "        )\n",
        "\n",
        "        self.flatten = nn.Flatten()\n",
        "        # Le calcul change : 128 filtres * image de 8x8 pixels = 8192 entrées\n",
        "        self.fc = nn.Linear(128 * 8 * 8, 2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.flatten(x)\n",
        "        return self.fc(x)\n",
        "\n",
        "# Initialisation du modèle profond\n",
        "model_base = DeepCNN().to(device)\n",
        "# On garde l'optimiseur Adam qui a très bien fonctionné\n",
        "optimizer= torch.optim.Adam(params=model_base.parameters(), lr=0.0001)"
      ],
      "metadata": {
        "id": "kYKZv_UFn-CE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 10\n",
        "train_losses_adam = []\n",
        "test_accuracies_adam = []\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    model_base.train()\n",
        "    train_loss = 0\n",
        "    for batch, (X, y) in enumerate(train_loader):\n",
        "        X, y = X.to(device), y.to(device)\n",
        "        y_pred = model_base(X)\n",
        "        loss = loss_fn(y_pred, y)\n",
        "        train_loss += loss.item()\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    # Phase de Test\n",
        "    model_base.eval()\n",
        "    test_acc = 0\n",
        "    with torch.inference_mode():\n",
        "        for X_test, y_test in test_loader:\n",
        "            X_test, y_test = X_test.to(device), y_test.to(device)\n",
        "            test_pred_logits = model_base(X_test)\n",
        "            test_acc += (torch.argmax(test_pred_logits, dim=1) == y_test).sum().item()\n",
        "\n",
        "    train_loss /= len(train_loader)\n",
        "    test_acc /= len(test_data)\n",
        "\n",
        "    train_losses_adam.append(train_loss)\n",
        "    test_accuracies_adam.append(test_acc)\n",
        "\n",
        "    print(f\"Époque: {epoch+1} | Perte: {train_loss:.4f} | Précision Test: {test_acc*100:.2f}%\")"
      ],
      "metadata": {
        "id": "f52ucNc0rmHt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}